{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyIBU1Y6U0L-"
      },
      "outputs": [],
      "source": [
        "!pip install wikipedia-api\n",
        "!pip install arxiv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "import arxiv\n",
        "\n",
        "def fetch_wikipedia_content(keyword):\n",
        "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"  # Replace with your actual user agent\n",
        "    headers = {'User-Agent': user_agent}\n",
        "\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('en', headers=headers)\n",
        "    page = wiki_wiki.page(keyword)\n",
        "    if not page.exists():\n",
        "        return \"\"\n",
        "    return page.text\n",
        "\n",
        "def fetch_arxiv_content(keyword):\n",
        "    papers = arxiv.query(query=keyword, max_results=5)  # Fetch a maximum of 5 papers\n",
        "    content = \"\"\n",
        "    for paper in papers:\n",
        "        content += f\"Title: {paper.title}\\nSummary: {paper.summary}\\n\\n\"\n",
        "    return content\n",
        "\n",
        "def create_dataset(keyword, output_filename):\n",
        "    wiki_content = fetch_wikipedia_content(keyword)\n",
        "    arxiv_content = fetch_arxiv_content(keyword)\n",
        "\n",
        "    combined_content = wiki_content + '\\n' + arxiv_content\n",
        "\n",
        "    with open(output_filename, 'w') as file:\n",
        "        file.write(combined_content)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    keyword = input(\"Enter a tag or keyword: \")\n",
        "    output_filename = 'dataset.txt'\n",
        "    create_dataset(keyword, output_filename)\n",
        "    print(\"Dataset created and saved as\", output_filename)\n"
      ],
      "metadata": {
        "id": "kRdu_EaxVEZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "\n",
        "def fetch_wikipedia_content(keyword):\n",
        "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"  # Replace with your actual user agent\n",
        "    headers = {'User-Agent': user_agent}\n",
        "\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('en', headers=headers)\n",
        "    page = wiki_wiki.page(keyword)\n",
        "    if not page.exists():\n",
        "        return \"\"\n",
        "    return page.text\n",
        "\n",
        "def create_dataset(keyword, output_filename):\n",
        "    wiki_content = fetch_wikipedia_content(keyword)\n",
        "\n",
        "    with open(output_filename, 'w') as file:\n",
        "        file.write(wiki_content)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    keyword = input(\"Enter a tag or keyword: \")\n",
        "    output_filename = 'wikipedia_dataset.txt'\n",
        "    create_dataset(keyword, output_filename)\n",
        "    print(\"Dataset created and saved as\", output_filename)\n"
      ],
      "metadata": {
        "id": "9O0xQpTiVEdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'wikipedia_dataset.txt'\n",
        "\n",
        "with open(filename, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "print(content)\n"
      ],
      "metadata": {
        "id": "Rxz8gwtyVEi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "\n",
        "def fetch_wikipedia_content(keyword):\n",
        "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\"  # Replace with your actual user agent\n",
        "    headers = {'User-Agent': user_agent}\n",
        "\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('en', headers=headers)\n",
        "\n",
        "    keyword_page = wiki_wiki.page(keyword)\n",
        "    if not keyword_page.exists():\n",
        "        return \"\"\n",
        "\n",
        "    content = \"\"\n",
        "    for link in keyword_page.backlinks.values():  # Fetch all pages that link to the keyword page\n",
        "        content += link.text + '\\n\\n'\n",
        "\n",
        "    return content\n",
        "\n",
        "def create_dataset(keyword, output_filename):\n",
        "    wiki_content = fetch_wikipedia_content(keyword)\n",
        "\n",
        "    with open(output_filename, 'w') as file:\n",
        "        file.write(wiki_content)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    keyword = input(\"Enter a tag or keyword: \")\n",
        "    output_filename = 'wikipedia_dataset.txt'\n",
        "    create_dataset(keyword, output_filename)\n",
        "    print(\"Dataset created and saved as\", output_filename)\n"
      ],
      "metadata": {
        "id": "uHy7I7O_VIIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VYF3Eh1gVIKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vaNtoDZJVIOM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}